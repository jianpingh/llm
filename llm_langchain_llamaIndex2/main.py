from langchain_community.embeddings import HuggingFaceEmbeddings  # âœ… æ³¨æ„æ–°ç‰ˆè·¯å¾„
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings

# 1ï¸âƒ£ ä½¿ç”¨ HuggingFace æœ¬åœ° Embeddingsï¼ˆGPU è‡ªåŠ¨æ”¯æŒï¼‰
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 2ï¸âƒ£ å°† Embedding è®¾ç½®ä¸º LlamaIndex é»˜è®¤åµŒå…¥å™¨
Settings.embed_model = embeddings

# 3ï¸âƒ£ åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("data").load_data()

# 4ï¸âƒ£ åˆ›å»ºç´¢å¼•å¹¶æ„å»ºæŸ¥è¯¢å¼•æ“
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()

# 5ï¸âƒ£ ç”¨æˆ·é—®é¢˜
user_question = "ä»€ä¹ˆæ˜¯å¿ƒç†å¥åº·ï¼Ÿ"

# 6ï¸âƒ£ æ£€ç´¢ä¸Šä¸‹æ–‡
retrieved_context = query_engine.query(user_question).response

# 7ï¸âƒ£ LangChain LLMï¼ˆèµ°ä»£ç†ï¼‰
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,
    api_key="sk-YL99cPdQUr7hbYg6ciOucPyN3NF7tgsz08DDF8lyDwPwfst1",
    base_url="https://api.chatanywhere.tech/v1"
)
# 8ï¸âƒ£ æ„å»ºæç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªå¿ƒç†å¥åº·é¢†åŸŸçš„ä¸“å®¶ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜æ—¶è¦ä¸“ä¸šã€ç®€æ´ã€‚"),
    ("user", "é—®é¢˜ï¼š{question}\n\nç›¸å…³ä¸Šä¸‹æ–‡ï¼š{context}")
])

# 9ï¸âƒ£ è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
final_prompt = prompt.format_messages(
    question=user_question,
    context=retrieved_context
)
response = llm.invoke(final_prompt)

# ğŸ”Ÿ è¾“å‡ºç»“æœ
print("=== LlamaIndex æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ ===")
print(retrieved_context)
print("\n=== LangChain + LlamaIndex ç”Ÿæˆçš„æœ€ç»ˆå›ç­” ===")
print(response.content)
